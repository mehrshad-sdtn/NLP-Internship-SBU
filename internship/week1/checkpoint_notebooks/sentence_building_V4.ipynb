{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "uPK2QfZkXcUv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "om4bi94uXz6W",
    "outputId": "807ac602-d13d-4d24-a46d-0dbcd157c8f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSoDE3KJX9bu",
    "outputId": "ed6efd35-0bf1-413d-a819-65f65f086b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jrRw5TNCYQDV"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = '/saved_models/nlp_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrBcoMhTY5jO",
    "outputId": "35dcd2c5-3042-42a9-8d4b-28579107acfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting hazm\n",
      "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
      "\u001b[K     |████████████████████████████████| 316 kB 32.7 MB/s \n",
      "\u001b[?25hCollecting libwapiti>=0.2.1\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 61.4 MB/s \n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 16.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=fa6736b2e7da860bba6fc63fac6b0855db009d69ea598987c714e149ea0f0669\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153428 sha256=b6d8a381141e1643b7481abb94a2fd9164fca3cb2bfd5495336e810b73493194\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P7WzGkNeJz0"
   },
   "source": [
    "###**Text preprocessing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DgLYXfMbY7r9"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import re\n",
    "import random\n",
    "from string import punctuation\n",
    "\n",
    "def text_preprocess(text):\n",
    "  normalizer = Normalizer()\n",
    "  text = normalizer.normalize(text)\n",
    "  text = re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "  return text\n",
    "\n",
    "def text_scramble(text):\n",
    "  words = text.split()\n",
    "  random.shuffle(words)\n",
    "  return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOZLJB5ieOSs"
   },
   "source": [
    "###**Loading and processing Corpus 1**\n",
    "(VOA FARSI 2003-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JBWO5KxPeITs"
   },
   "outputs": [],
   "source": [
    "path = 'data/voa_fa_2003-2008_orig.txt'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "  text = ' '.join([line.strip() for line in f.readlines() if not line.startswith('#')])\n",
    "  text = text.split('.')\n",
    "  sents = random.sample(text, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jXPJ0lLqeueE"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 170\n",
    "sents_cleaned = [text_preprocess(s) for s in sents]\n",
    "sents_ready = filter(lambda s: len(s) < MAX_LEN, sents_cleaned)\n",
    "sents_data = list(sents_ready)\n",
    "random.shuffle(sents_data)\n",
    "sents_data = sents_data[:20000]  # make dataset size 20000 for ease of computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aZu1i3fZ4jaF"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(sents_data)\n",
    "tokenizer.word_index['<PAD>'] = 0\n",
    "last_idx = len(tokenizer.word_index) + 1\n",
    "tokenizer.word_index['<SOS>'] = last_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8S8uD4Lp_GGF",
    "outputId": "c5bdb120-c6e0-4ae1-cac5-d9a4384d3099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum sentence length: 42\n"
     ]
    }
   ],
   "source": [
    "temp = [len(each.split()) for each in sents_data]\n",
    "print('maximum sentence length:', sorted(temp)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hjb7_snihFC"
   },
   "source": [
    "### **building the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vw9NSMqyiDDL"
   },
   "outputs": [],
   "source": [
    "def create_shuffles(sent_list, m):\n",
    "  \"\"\"\n",
    "  this function creates m random shuffles of the sentence\n",
    "  \"\"\"\n",
    "  all_combs = []\n",
    "  for sent in sent_list:\n",
    "    comb_set = set([])\n",
    "    for i in range(0, m + 1):\n",
    "      comb_set.add(text_scramble(sent))\n",
    "    all_combs.append((sent, comb_set))\n",
    "  return all_combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tIM26pJyjBDr"
   },
   "outputs": [],
   "source": [
    "shuffle_pairs = create_shuffles(sents_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kh5Bf4KHjh1e",
    "outputId": "cf42e69a-2a5c-4e0e-ff64-e6a1fa7f57e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' یک تصادف رانندگی به علت شدت باد و دود ناشی از آتش سوزی در بزرگراه ۱۱۸ باعث مرگ راننده این اتومبیل و مجروح شدن سرنشینان دیگر شد',\n",
       " {'اتومبیل ۱۱۸ در و علت مجروح سرنشینان راننده از دود و باعث ناشی یک مرگ شدن تصادف این باد شد دیگر بزرگراه رانندگی به آتش شدت سوزی',\n",
       "  'این ۱۱۸ سوزی بزرگراه علت در آتش به دیگر یک ناشی شدت سرنشینان از و مرگ رانندگی و اتومبیل باعث باد شدن شد راننده تصادف دود مجروح',\n",
       "  'باعث ناشی در دیگر سوزی شدت دود این از آتش مجروح علت و سرنشینان شد باد شدن رانندگی مرگ بزرگراه به تصادف راننده یک ۱۱۸ و اتومبیل',\n",
       "  'بزرگراه اتومبیل و باد سرنشینان دود باعث و به یک از تصادف شدت آتش شد این دیگر ناشی رانندگی شدن در راننده ۱۱۸ مجروح مرگ سوزی علت',\n",
       "  'در سوزی آتش دیگر و بزرگراه باد شدت و به مجروح تصادف یک ۱۱۸ مرگ شد ناشی شدن دود علت این رانندگی سرنشینان باعث راننده از اتومبیل',\n",
       "  'در یک تصادف رانندگی باعث اتومبیل بزرگراه علت و سوزی شدت شد شدن آتش باد ناشی دود از مرگ دیگر و ۱۱۸ سرنشینان راننده مجروح این به',\n",
       "  'راننده دود دیگر علت مرگ از ناشی سرنشینان شد به باعث رانندگی این آتش و سوزی مجروح در شدت باد تصادف یک شدن و بزرگراه ۱۱۸ اتومبیل',\n",
       "  'مجروح ۱۱۸ این آتش به علت بزرگراه شد شدن در شدت رانندگی و دیگر اتومبیل راننده و دود مرگ تصادف ناشی باد باعث سرنشینان یک از سوزی',\n",
       "  'ناشی یک علت سرنشینان و دیگر شد دود تصادف ۱۱۸ این آتش سوزی رانندگی از مجروح در شدن و به بزرگراه باد اتومبیل شدت مرگ راننده باعث',\n",
       "  'و آتش شدت راننده مجروح یک از تصادف شدن دیگر سوزی دود در این سرنشینان باد باعث ۱۱۸ مرگ به ناشی علت بزرگراه شد رانندگی و اتومبیل',\n",
       "  '۱۱۸ اتومبیل به و مجروح این از در علت دود یک سرنشینان شدن شد باعث باد رانندگی تصادف سوزی آتش دیگر ناشی بزرگراه و راننده مرگ شدت'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_pairs[1][0], shuffle_pairs[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wnoA5xmDjrBU"
   },
   "outputs": [],
   "source": [
    "def create_dataframe(pairs):\n",
    "  \"\"\"\n",
    "  creates a dataframe with two columns of original sentence and it's shuffles\n",
    "  \"\"\"\n",
    "  original = []\n",
    "  shuffled = []\n",
    "  for pair in pairs:\n",
    "    for p in pair[1]:\n",
    "      original.append(pair[0])\n",
    "      shuffled.append(p)\n",
    "\n",
    "  df_dict = { 'Shuffled': shuffled, 'Original': original}\n",
    "  df = pd.DataFrame(df_dict)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "jTZJ1mSbkXuy",
    "outputId": "15b01480-5033-4b0f-97e4-48e2d54ce9a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9f5aea34-3fbc-499b-8450-7af2b99f302b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shuffled</th>\n",
       "      <th>Original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>است آسیب حاکی کرده خبری شوهر بازداشت داده است ...</td>\n",
       "      <td>گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>کرده شوهر حاکی گزارش‌های مورد نروژی و زن را ند...</td>\n",
       "      <td>گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>گزارش‌های که ندید است بازجوئی خبری بازداشت را ...</td>\n",
       "      <td>گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مورد زن شوهر ندید نروژی بازجوئی را است که قرار...</td>\n",
       "      <td>گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>داده گزارش‌های است آسیب است مورد و زن شوهر حاک...</td>\n",
       "      <td>گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217021</th>\n",
       "      <td>شد دوباره به نفر آوردن مقام آغاسی اول پیرترین ...</td>\n",
       "      <td>با بدست آوردن دوباره این مقام آغاسی جایگزین ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217022</th>\n",
       "      <td>این بدست مقام به شد پیرترین نفر آوردن جایگزین ...</td>\n",
       "      <td>با بدست آوردن دوباره این مقام آغاسی جایگزین ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217023</th>\n",
       "      <td>با عنوان به در مقام دوباره تنیس کانرز پیرترین ...</td>\n",
       "      <td>با بدست آوردن دوباره این مقام آغاسی جایگزین ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217024</th>\n",
       "      <td>اول کانرز تنیس دنیا آوردن به جیمی شد عنوان دوب...</td>\n",
       "      <td>با بدست آوردن دوباره این مقام آغاسی جایگزین ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217025</th>\n",
       "      <td>آوردن جیمی شد بدست عنوان نفر به مقام آغاسی تنی...</td>\n",
       "      <td>با بدست آوردن دوباره این مقام آغاسی جایگزین ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217026 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f5aea34-3fbc-499b-8450-7af2b99f302b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9f5aea34-3fbc-499b-8450-7af2b99f302b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9f5aea34-3fbc-499b-8450-7af2b99f302b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                 Shuffled  \\\n",
       "0       است آسیب حاکی کرده خبری شوهر بازداشت داده است ...   \n",
       "1       کرده شوهر حاکی گزارش‌های مورد نروژی و زن را ند...   \n",
       "2       گزارش‌های که ندید است بازجوئی خبری بازداشت را ...   \n",
       "3       مورد زن شوهر ندید نروژی بازجوئی را است که قرار...   \n",
       "4       داده گزارش‌های است آسیب است مورد و زن شوهر حاک...   \n",
       "...                                                   ...   \n",
       "217021  شد دوباره به نفر آوردن مقام آغاسی اول پیرترین ...   \n",
       "217022  این بدست مقام به شد پیرترین نفر آوردن جایگزین ...   \n",
       "217023  با عنوان به در مقام دوباره تنیس کانرز پیرترین ...   \n",
       "217024  اول کانرز تنیس دنیا آوردن به جیمی شد عنوان دوب...   \n",
       "217025  آوردن جیمی شد بدست عنوان نفر به مقام آغاسی تنی...   \n",
       "\n",
       "                                                 Original  \n",
       "0        گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...  \n",
       "1        گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...  \n",
       "2        گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...  \n",
       "3        گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...  \n",
       "4        گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را...  \n",
       "...                                                   ...  \n",
       "217021    با بدست آوردن دوباره این مقام آغاسی جایگزین ...  \n",
       "217022    با بدست آوردن دوباره این مقام آغاسی جایگزین ...  \n",
       "217023    با بدست آوردن دوباره این مقام آغاسی جایگزین ...  \n",
       "217024    با بدست آوردن دوباره این مقام آغاسی جایگزین ...  \n",
       "217025    با بدست آوردن دوباره این مقام آغاسی جایگزین ...  \n",
       "\n",
       "[217026 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_dataframe(shuffle_pairs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HRkeMJafwY7_"
   },
   "outputs": [],
   "source": [
    "def create_dataset(df, train_percent=80):\n",
    "  data_size = df.shape[0]\n",
    "  count = math.floor(data_size * (train_percent/100))\n",
    "\n",
    "  dataset = df['Shuffled'].values\n",
    "  labels  = df['Original'].values\n",
    "\n",
    "  train_data   = dataset[:count]\n",
    "  train_labels = labels[:count]\n",
    "  test_data   = dataset[count:]\n",
    "  test_labels = labels[count:]\n",
    "\n",
    "  return ((train_data, train_labels), (test_data, test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Dh3otPKB0GrV"
   },
   "outputs": [],
   "source": [
    "train_set, test_set = create_dataset(df)\n",
    "X_train_text, Y_train_text = train_set\n",
    "X_test_text, Y_test_text = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vq7ch6G1jX1"
   },
   "source": [
    "###**making dataset ready for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "y9Y7Kqc4w1XV"
   },
   "outputs": [],
   "source": [
    "sos_index = last_idx\n",
    "def shift_output_sequence(seq_list):\n",
    "  for seq in seq_list:\n",
    "    seq.insert(0, sos_index)\n",
    "  return seq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZPoeo0sYwWPO"
   },
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "Y_train_seq = tokenizer.texts_to_sequences(Y_train_text)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(X_test_text)\n",
    "Y_test_seq  = tokenizer.texts_to_sequences(Y_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UkcJ_sWk1rKF"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 45  \n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN)\n",
    "Y_train = keras.preprocessing.sequence.pad_sequences(Y_train_seq, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "X_test  = keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN)\n",
    "Y_test  = keras.preprocessing.sequence.pad_sequences(Y_test_seq, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "X_train_decoder = keras.preprocessing.sequence.pad_sequences(shift_output_sequence(Y_train_seq), maxlen=MAX_SEQ_LEN)\n",
    "X_test_decoder  = keras.preprocessing.sequence.pad_sequences(shift_output_sequence(Y_test_seq),  maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MsYgFws1eJE",
    "outputId": "0552e913-4177-4fae-cc1b-de86f9fb9d01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 24248,   873,   380,\n",
       "           494,     8,    73,  4432,   451,  4131,     6,     7,   553,\n",
       "          3627,   202,    29,     4,    40,  1758,    47,    89,     8],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 24248,   873,   380,\n",
       "           494,     8,    73,  4432,   451,  4131,     6,     7,   553,\n",
       "          3627,   202,    29,     4,    40,  1758,    47,    89,     8]],\n",
       "       dtype=int32),\n",
       " array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 24248,   586,\n",
       "           234,  1708,     2,  2507,   530,    79,  5155,     2,    31,\n",
       "          9010,     3,    11,   918,  2980, 21714,    43,   737,    23],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 24248,   586,\n",
       "           234,  1708,     2,  2507,   530,    79,  5155,     2,    31,\n",
       "          9010,     3,    11,   918,  2980, 21714,    43,   737,    23]],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder[:2], X_test_decoder[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jR3JqKVK96iy",
    "outputId": "73e71aea-127b-47a5-8806-ba69b1dd5cae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> است آسیب حاکی کرده خبری شوهر بازداشت داده است ندید زن را مورد قرار گزارش\\u200cهای و بازجوئی نروژی پلیس که'],\n",
       " ['<UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> گزارش\\u200cهای خبری حاکی است پلیس شوهر زن نروژی را که آسیب ندید بازداشت کرده و مورد بازجوئی قرار داده است'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(X_train[:1]), tokenizer.sequences_to_texts(Y_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5trFrkKtX8JD",
    "outputId": "56100832-7384-4f38-b1ae-c3e6e9030f10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24248"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3F9foXMQ7BK"
   },
   "source": [
    "###**Encoder-Decoder network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHYgCVUBWhXl",
    "outputId": "4e077123-4b8c-4ead-de24-5fbb83482542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 32)     775968      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 32)     776000      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 128),        82432       ['embedding[0][0]']              \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, None, 128)    82432       ['embedding_1[0][0]',            \n",
      "                                                                  'lstm[0][1]',                   \n",
      "                                                                  'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 24249)  3128121     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,844,953\n",
      "Trainable params: 4,844,953\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "encoder_embedding = keras.layers.Embedding(input_dim=VOCAB_SIZE + 1,output_dim=encoder_embedding_size, input_length=MAX_SEQ_LEN, mask_zero=True)(encoder_input)\n",
    "\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(lstm_units, dropout=0.2, return_state=True, recurrent_regularizer='l2')(encoder_embedding)\n",
    "\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "decoder_embedding = keras.layers.Embedding(input_dim=VOCAB_SIZE + 2, output_dim=decoder_embedding_size, mask_zero=True)(decoder_input)\n",
    "\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, dropout=0.2, return_sequences=True, recurrent_regularizer='l2')(decoder_embedding, initial_state=encoder_state)\n",
    "\n",
    "decoder_output = keras.layers.Dense(VOCAB_SIZE + 1, activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbRGiGpaW_-R",
    "outputId": "c7a1d25f-21fc-4fde-a024-e83ccab28c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "272/272 [==============================] - 184s 676ms/step - loss: 3.4998 - accuracy: 0.0782 - val_loss: 3.0202 - val_accuracy: 0.0808\n",
      "Epoch 2/30\n",
      "272/272 [==============================] - 183s 673ms/step - loss: 2.7111 - accuracy: 0.1130 - val_loss: 2.4847 - val_accuracy: 0.1860\n",
      "Epoch 3/30\n",
      "272/272 [==============================] - 183s 672ms/step - loss: 2.0178 - accuracy: 0.3285 - val_loss: 1.7223 - val_accuracy: 0.5233\n",
      "Epoch 4/30\n",
      "272/272 [==============================] - 183s 673ms/step - loss: 1.3267 - accuracy: 0.6116 - val_loss: 1.1621 - val_accuracy: 0.7422\n",
      "Epoch 5/30\n",
      "272/272 [==============================] - 183s 673ms/step - loss: 0.8829 - accuracy: 0.7679 - val_loss: 0.8712 - val_accuracy: 0.8351\n",
      "Epoch 6/30\n",
      "272/272 [==============================] - 183s 672ms/step - loss: 0.6256 - accuracy: 0.8386 - val_loss: 0.7197 - val_accuracy: 0.8758\n",
      "Epoch 7/30\n",
      "272/272 [==============================] - 184s 677ms/step - loss: 0.4590 - accuracy: 0.8806 - val_loss: 0.6352 - val_accuracy: 0.9005\n",
      "Epoch 8/30\n",
      "272/272 [==============================] - 185s 681ms/step - loss: 0.3395 - accuracy: 0.9090 - val_loss: 0.5850 - val_accuracy: 0.9144\n",
      "Epoch 9/30\n",
      "272/272 [==============================] - 184s 676ms/step - loss: 0.2495 - accuracy: 0.9310 - val_loss: 0.5588 - val_accuracy: 0.9225\n",
      "Epoch 10/30\n",
      "272/272 [==============================] - 182s 668ms/step - loss: 0.1812 - accuracy: 0.9530 - val_loss: 0.5402 - val_accuracy: 0.9274\n",
      "Epoch 11/30\n",
      "272/272 [==============================] - 181s 665ms/step - loss: 0.1306 - accuracy: 0.9726 - val_loss: 0.5338 - val_accuracy: 0.9308\n",
      "Epoch 12/30\n",
      "272/272 [==============================] - 180s 661ms/step - loss: 0.0947 - accuracy: 0.9840 - val_loss: 0.5286 - val_accuracy: 0.9333\n",
      "Epoch 13/30\n",
      "272/272 [==============================] - 179s 660ms/step - loss: 0.0704 - accuracy: 0.9895 - val_loss: 0.5251 - val_accuracy: 0.9351\n",
      "Epoch 14/30\n",
      "272/272 [==============================] - 180s 660ms/step - loss: 0.0539 - accuracy: 0.9922 - val_loss: 0.5235 - val_accuracy: 0.9365\n",
      "Epoch 15/30\n",
      "272/272 [==============================] - 180s 662ms/step - loss: 0.0424 - accuracy: 0.9938 - val_loss: 0.5224 - val_accuracy: 0.9372\n",
      "Epoch 16/30\n",
      "272/272 [==============================] - 184s 677ms/step - loss: 0.0343 - accuracy: 0.9949 - val_loss: 0.5219 - val_accuracy: 0.9380\n",
      "Epoch 17/30\n",
      "272/272 [==============================] - 186s 682ms/step - loss: 0.0283 - accuracy: 0.9956 - val_loss: 0.5208 - val_accuracy: 0.9385\n",
      "Epoch 18/30\n",
      "272/272 [==============================] - 185s 680ms/step - loss: 0.0239 - accuracy: 0.9961 - val_loss: 0.5203 - val_accuracy: 0.9391\n",
      "Epoch 19/30\n",
      "272/272 [==============================] - 184s 677ms/step - loss: 0.0205 - accuracy: 0.9965 - val_loss: 0.5195 - val_accuracy: 0.9393\n",
      "Epoch 20/30\n",
      "272/272 [==============================] - 184s 677ms/step - loss: 0.0175 - accuracy: 0.9970 - val_loss: 0.5172 - val_accuracy: 0.9398\n",
      "Epoch 21/30\n",
      "272/272 [==============================] - 184s 676ms/step - loss: 0.0153 - accuracy: 0.9973 - val_loss: 0.5185 - val_accuracy: 0.9403\n",
      "Epoch 22/30\n",
      "272/272 [==============================] - 183s 675ms/step - loss: 0.0136 - accuracy: 0.9974 - val_loss: 0.5174 - val_accuracy: 0.9406\n",
      "Epoch 23/30\n",
      "272/272 [==============================] - 184s 676ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.5166 - val_accuracy: 0.9409\n",
      "Epoch 24/30\n",
      "272/272 [==============================] - 184s 675ms/step - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.5176 - val_accuracy: 0.9413\n",
      "Epoch 25/30\n",
      "272/272 [==============================] - 183s 674ms/step - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.5156 - val_accuracy: 0.9413\n",
      "Epoch 26/30\n",
      "272/272 [==============================] - 184s 675ms/step - loss: 0.0086 - accuracy: 0.9982 - val_loss: 0.5178 - val_accuracy: 0.9417\n",
      "Epoch 27/30\n",
      "272/272 [==============================] - 184s 676ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.5159 - val_accuracy: 0.9419\n",
      "Epoch 28/30\n",
      "272/272 [==============================] - 184s 676ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.5164 - val_accuracy: 0.9422\n",
      "Epoch 29/30\n",
      "272/272 [==============================] - 183s 675ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.5144 - val_accuracy: 0.9424\n",
      "Epoch 30/30\n",
      "272/272 [==============================] - 183s 673ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.5157 - val_accuracy: 0.9424\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=30, validation_split=0.2, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KafwsaNYH7wx"
   },
   "source": [
    "**almost 100% accuracy on training data after 30 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YcZe294WJs2u"
   },
   "outputs": [],
   "source": [
    "model.save(\"sentence_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdvpqp9WRAqC"
   },
   "source": [
    "###**Simple seq2seq network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtOseVVsIx8W"
   },
   "source": [
    "**model evaluation on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "n8H6DWLiNMPM"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"sentence_model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-91S1Qjfxjoh",
    "outputId": "5b1d8224-4dda-4091-aa3f-6ab17095f7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651,\n",
       "         5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651, 5651,\n",
       "         5651, 5651, 5651,    0,  586,  234, 1708,    2, 2507,  530,   79,\n",
       "         5155,    2,   31, 9010,    3,   11,  918, 2980, 2980,   43,  737,\n",
       "           23]]),\n",
       " array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,   586,\n",
       "           234,  1708,     2,  2507,   530,    79,  5155,     2,    31,\n",
       "          9010,     3,    11,   918,  2980, 21714,    43,   737,    23]],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.argmax(model.predict([X_test[:1], X_test_decoder[:1]]), axis=-1)\n",
    "ids, Y_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJb6gcXWI5Vn",
    "outputId": "21405f8f-3f59-4fdb-d9e0-df6a528086e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6553 - accuracy: 0.9442\n",
      "test loss, test acc: [0.6552770137786865, 0.9441624283790588]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate([X_test[:10], X_test_decoder[:10]], Y_test[:10], batch_size=512)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7YoZLmqOKpN"
   },
   "source": [
    "###**Rebuild sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "qPrbtyI2yRg4"
   },
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "def remove_padding_and_join(texts):\n",
    "  for i in range(len(texts)):\n",
    "    no_padding = filter(lambda a: a != '<PAD>', texts[i])\n",
    "    texts[i] = ' '.join(list(no_padding))\n",
    "  return texts\n",
    "    \n",
    "\n",
    "def reconstruct_predicted_list(texts):\n",
    "  for i in range(len(texts)):\n",
    "    pad_start = texts[i].index('<PAD>')\n",
    "    texts[i] = texts[i][pad_start:]\n",
    "    no_padding = filter(lambda a: a != '<PAD>', texts[i])\n",
    "    texts[i] = ' '.join(list(no_padding))\n",
    "  return texts\n",
    "\n",
    "def get_predictions_on_test(count):\n",
    "    Y_preds = model.predict([X_test[:count], X_test_decoder[:count]])\n",
    "    encoded_argmax  = np.argmax(Y_preds, axis=-1)\n",
    "    return encoded_argmax, Y_test[:count]\n",
    "\n",
    "def get_predicted_sentences(predicted):\n",
    "  predicted_word_list = list(map(sequence_to_text, predicted))\n",
    "  predicted_sent_list = reconstruct_predicted_list(predicted_word_list)\n",
    "  return predicted_sent_list\n",
    "\n",
    "def get_original_sentences(original):\n",
    "  original_word_list  = list(map(sequence_to_text, original))\n",
    "  original_sent_list = remove_padding_and_join(original_word_list)\n",
    "  return original_sent_list \n",
    "\n",
    "\n",
    "def rebuild_sentence_from_sequence(count=20):\n",
    "  predicted, original = get_predictions_on_test(count)\n",
    "  predicted_sent_list = get_predicted_sentences(predicted)\n",
    "  original_sent_list  = get_original_sentences(original)\n",
    "  return predicted_sent_list, original_sent_list\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wRouJWAOOLV",
    "outputId": "75c82402-aaba-43d8-91bb-dc117181b9a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای طلای دست یافته بود',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'مأمورین رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'مأمورین رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'انفجارروز رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'مأمورین رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد'],\n",
       " ['آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آخرین بار آرژانتین در بازی\\u200cهای المپیک شهر هلسینکی در سال ۱۹۵۲ به یک مدال طلای پاروزنی دست یافته بود',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد',\n",
       "  'آبل گوبادیا رئیس کمیسیون انتخابات نیجریه در ساعات پایانی سه شنبه این خبر را اعلام کرد'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, origs = rebuild_sentence_from_sequence()\n",
    "preds, origs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "gujI95KC97_X"
   },
   "outputs": [],
   "source": [
    "def predict_a_sentence(sentence):\n",
    "  seq = tokenizer.texts_to_sequences([sentence])\n",
    "  X = keras.preprocessing.sequence.pad_sequences(seq, maxlen=MAX_SEQ_LEN)\n",
    "  X_decoder = keras.preprocessing.sequence.pad_sequences(shift_output_sequence(seq), maxlen=MAX_SEQ_LEN)\n",
    "  Y_pred = model.predict([X, X_decoder])\n",
    "  encoded_argmax  = np.argmax(Y_pred, axis=-1)\n",
    "\n",
    "  output = get_predicted_sentences(encoded_argmax)\n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6bC8gO1FoOT",
    "outputId": "9b18e5c8-699e-40d4-921e-a1c0d5075f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['است آسیب حاکی کرده خبری شوهر بازداشت داده است ندید زن را مورد قرار گزارش\\u200cهای و بازجوئی نروژی پلیس که']\n",
      " گزارش‌های خبری حاکی است پلیس شوهر زن نروژی را که آسیب ندید بازداشت کرده و مورد بازجوئی قرار داده است\n",
      "است آسیب حاکی کرده خبری شوهر بازداشت داده است ندید زن را مورد قرار گزارش‌های و بازجوئی نروژی پلیس که\n",
      "----\n",
      "shuffled: \n",
      " [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0   737    11     2    43    31    23  1708 21714   586  2980\n",
      "    918    79     2   234  5155  2507   530  9010     3]] \n",
      "\n",
      "predicted: \n",
      " [[5651 5651 5651 5651 5651 5651 5651 5651 5651 5651 5651 5651 5651 5651\n",
      "  5651 5651 5651 5651 5651 5651 5651 5651 5651 5651 5651    0  586  234\n",
      "  1708    2 2507  530   79 5155    2   31 9010    3   11  918 2980 2980\n",
      "    43  737   23]] \n",
      "\n",
      "original: \n",
      " [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0   586   234  1708     2  2507   530    79  5155     2    31\n",
      "   9010     3    11   918  2980 21714    43   737    23]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predict_a_sentence(X_train_text[0]))\n",
    "print(Y_train_text[0])\n",
    "print(X_train_text[0])\n",
    "\n",
    "print('----')\n",
    "\n",
    "print('shuffled: \\n', X_test[:1], '\\n')\n",
    "print('predicted: \\n', np.argmax(model.predict([X_test[:1], X_test_decoder[:1]]), axis=-1), '\\n')\n",
    "print('original: \\n', Y_test[:1], '\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentence_building.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
